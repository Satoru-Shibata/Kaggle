# 20 Data Science Publications
## Executive Summary
### About the Author
- Name: Satoru Shibata / 柴田 怜
- Job: Data Scientist
  - Title: [Kaggle Triple Expert Top 0.2%, 0.3%, 1%](https://github.com/satorushibata0627/Kaggle/raw/main/Kaggle_Triple_Expert_Evidence.pdf)
    - Programming: Python/R
    - Current Status: Retirement from Kaggle to focus on job of mine
   - 8 Stat Certificates
     - [Deep Learning for Business](https://github.com/Satoru-Shibata-JP/Certification/raw/main/20220307_Coursera_Deep_Learning_for_Business.pdf)
     - [Practical Time Series Analysis](https://github.com/Satoru-Shibata-JP/Certification/raw/main/20210917_Coursera_Practical_Time_Series_Analysis.pdf)
     - [Exploratory Data Analysis for Machine Learning](https://github.com/Satoru-Shibata-JP/Certification/raw/main/20220325_Coursera_Exploratory_Data_Analysis_for_Machine_Learning.pdf)
     - [Supervised Machine Learning Regression](https://github.com/Satoru-Shibata-JP/Certification/raw/main/20220328_Coursera_Supervised_Machine_Learning_Regression.pdf)
     - [Data Science Math Skills](https://github.com/Satoru-Shibata-JP/Certification/raw/main/20210916_Coursera_Data_Science_Math_Skills.pdf)
     - [Blockchain Cryptocurrency Explained](https://github.com/Satoru-Shibata-JP/Certification/raw/main/20220222_Coursera_Blockchain_Cryptocurrency_Certification.pdf)
     - MEcon in Causal Inference at Sophia University
        - [Total GPA 3.74/4.00 (93.5%)](https://github.com/Satoru-Shibata-JP/Publication/raw/main/%E4%B8%8A%E6%99%BA%E5%A4%A7%E5%AD%A6%E7%92%B0%E5%A2%83%E5%AD%A6%E4%BF%AE%E5%A3%AB%E6%88%90%E7%B8%BE%E8%A8%BC%E6%98%8E%E6%9B%B8.pdf)
        - Course: [Statistics](https://github.com/Satoru-Shibata-JP/Certification/raw/main/20171004_Statistical_Data_Analyst_of_The%20Institute_of_Statistical%20Science.pdf)
      - [SQL for Statistics Essential Training](https://github.com/Satoru-Shibata-JP/Certification/raw/main/CertificateOfCompletion_SQL%20for%20Statistics%20Essential%20Training.pdf)
- Institution: AVANT CORPORATION
  - JPX 3836

### Score Table
| Departments      | Top Levels| Highest Rank | Awarded Medals     | 
| :---:            | :-------: | :----------: | :----------------: |
| **Code**         | 0.2%      | 317/161,898  | 3 Silver 13 Bronze |
| **Discussion**   | 0.3%      | 588/188,433  | 100 Bronze         |
| **Datasets**     | 1%        | 354/34,643   | 3 Bronze           |
| **Competitions** | 20-30%

### Abstract 16 Awarded Analytical Reports posted on Kaggle 16GB Kernel Python
#### 3 Silver Medals
1. [Optimized LightGBM with Optuna adding SAKT Model](https://github.com/satorushibata0627/Kaggle/blob/main/Kaggle_Python3_Silver_Medal_Optimized_LightGBM_with_Optuna_adding_SAKT_Model.ipynb)
    - Lead sentences
      - Submitted code using two Ensemble Learning Methods.
      - Used 100 million rows of training data for prediction on a 16GB Kernel removing unnecessary objects.
    - Issue
      - Algorithms for TOEIC Learning Applications
    - Significance
      - Predict percentage of correct answers based on user's behavioral history.
      - User's percentage of correct answers will increase with the number of problems solved.
    - Purpose
      - Optimize Binary Classification for AUC.
    - Methodology:
      - Ensemble Learning of LightGBM and SAKT
      - Hyperparameter Optimization with Optuna
    - Results
      - Score: AUC = 0.781
      - Code: 31 Points
    - Considerations
      - Obsessed with Models, Feature Engineering Remains a Challenge.
      - Systematizing Multiple Models will also be a challenge in the future.
    - Conclusion
      - Code Silver Medal
2. [LightGBM Classifier and Logistic Regression Report](https://github.com/satorushibata0627/Kaggle/blob/main/Kaggle_Python3_Silver_Medal_LightGBM_Classifier_and_Logistic_Regression_Report.ipynb)
    - Lead sentences
      - Optimized Classification of Anonymized Raw Data from Stock Market on 16GB Kernel.
      - Contributed code that systematizes Ensemble Learning and Logistic Regression.
    - Issue
      - Utility Function Optimization of Supply and Demand Forecasting in Securities Markets
    - Significance
      - Calculate based on indicators of absence or degree of stock returns.
      - Optimize the behavior of whether to trade or not.
    - Purpose
      - AI Dev for Profit Maximization
    - Methodology
      - Optimal classification of LightGBM
      - Logit Transformation of Purpose Variables Based on Probability Distributions
    - Results
      - Score: 3741.118 (Outside of Medal Zone)
      - Code: 33 Points
    - Considerations
      - Utility function was not fully deciphered
      - Which left some issues for the paper survey.
      - I think that the reporting was appreciated by other Kaggler.
    - Conclusion
      - Code Silver Medal
3. [Optimize LightGBM HyperParameter with Optuna and GPU](https://github.com/satorushibata0627/Kaggle/blob/main/Kaggle_Python3_Silver_Medal_Optimize_LightGBM_HyperParameter_with_Optuna_and_GPU.ipynb)
    - Lead sentences
      - Unprecedented LightGBM Hyperparameter Optimization on GPU!
      - Procedure was annexed and highly evaluated.
    - Issues
      - Preliminaries of “LightGBM Classifier and Logistic Regression Report“ 
    - Significance
      - High Parameter Optimization
      - There were few precedents for LightGBM.
    - Purpose
      - Code submission for optimizing LightGBM Hyperparameter on GPU
    - Methodology
      - A survey of prior case studies using Optuna for LightGBM
      - Procedure Submission
    - Results
      - Run: 953.9s on GPU
      - Code: 31 Points
    - Consideration
      - It can be used for Hyperparameter Optimization in the future.
    - Conclusion
      - Code Silver Medal

#### 13 Bronze Medals
1. [Optimized Logit LightGBM Classifier and CNN Models](https://github.com/satorushibata0627/Kaggle/blob/main/Kaggle_Python3_Bronze_Medal_Optimized_Logit_LightGBM_Classifier_and_CNN_Model.ipynb)
    - Lead sentences
      - Submitted a simulation of Multiple Model Systematization.
      - Based on this failure, I was able to concentrate on LightGBM Optimization and Inference.
    - Issue
      - Exploring Optimization Models
    - Significance
      - Simulation iterations of the Optimization Model
    - Purpose
      - Optimize Utility Function by systematizing Multiple Models.
    - Methodology
      - Applying the Logit Transform to LightGBM
      - Explore combining with CNN
    - Results
      - Score: 3344.738 (Outside of Medal Zone)
      - Code: 15 Points
    - Considerations
      - This code does LightGBM and CNN at the same time, which was prone to overflow.
      - From now on, I will focus on one Model Optimization.
    - Conclusion
      - Code Bronze Medal
1. [Optimized LightGBM with Optuna](https://github.com/satorushibata0627/Kaggle/blob/main/Kaggle_Python3_Bronze_Medal_Optimized_LightGBM_with_Optuna.ipynb)
    - Lead sentences
      - Dev Baseline Model for Code Competition to process 100 million rows of training data.
      - The minimum performance was predicted to be 16GB.
    - Issue
      - 100 million rows of training data must be predicted on a 16GB Kernel.
    - Significance
      - This is the cornerstone of the final submission model.
      - Preprocess and Feature Engineering were adjusted for further optimization.
    - Purpose
      - Baseline Model Dev
    - Methodology
      - Binary Classification by LightGBM Optimization
    - Results
      - Score: AUC = 0.774
      - Code: 12 Points
    - Considerations
      - Policy of additional development to Baseline Model.
      - The improvement of AUC by the additional development was only 0.07, which left some issues.
    - Conclusion
      - Code Bronze Medal
1. [LightGBM on GPU with Feature Engineering, Optuna, and Visualization](https://github.com/satorushibata0627/Kaggle/blob/main/Kaggle_Python3_Bronze_Medal_LightGBM_on_GPU_with_Feature_Engineering_Optuna_and_Visualization.ipynb)
    - Lead sentence
      - I won a Code Bronze Medal for my first attempt at submitting code!
    - Issue
      - This was my first real effort at Kaggle.
    - Significance
      - Visualize in a timely manner, and features were studied.
      - Optuna was also used for the first time and applied later.
    - Purpose
      - Work on Feature Engineering.
    - Methodology
      - I read and referred to posted code by Kaggle Grandmaster.
    - Results
      - Code: 11 Points
    - Consideration
      - I could gain experiences in implementing LightGBM with Optuna on GPU.
    - Conclusion
      - Code Bronze Medal
1. [LightGBM with the Inference and Empirical Analysis](https://github.com/satorushibata0627/Kaggle/blob/main/Kaggle_Python3_Bronze_Medal_LightGBM_with_the_Inference_and_Empirical_Analysis.ipynb)
    - Lead sentences
      - In the first scored submission code, AUC = 0.76.
      - The challenges were used as the cornerstone of development experiences.
    - Issue
      - Scoring by developing additions to the submitted code for my first challenge.
    - Significance
      - A single process was limited to Model Object Generation.
    - Purpose
      - To further improve the performance of Prediction Model.
    - Methodology
      - Inference was added to improve Score.
      - Empirical Analysis between raw data and predicted results.
      - Detected significant differences in Gaussian Distribution.
    - Results
      - Score: AUC = 0.76
      - Code: 12 Points
    - Consideration
      - This submitted code left insufficient understanding of inference as an issue.
    - Conclusion
      - Code Bronze Medal
1. [Submission and the Inference of LightGBM](https://github.com/satorushibata0627/Kaggle/blob/main/Kaggle_Python3_Bronze_Medal_Submission_and_the_Inference_of_LightGBM.ipynb)
    - Lead sentences
      - My first scoring submission code prototype
      - Few examples of Empirical Analysis, I won Code Bronze Medal!
    - Issue
      - Prototype version of submission code for first scoring
    - Significance
      - Implementing the scoring submission code
    - Purpose
      - Gaining development experiences.
    - Methodology
      - Model objects were coded for scoring.
      - Empirical Analysis detected a significant difference in Gaussian Distribution.
    - Result
      - Code: 7 Points
    - Considerations
      - Actual scoring submission code became a separate file.
      - This was an opportunity for me to feel the challenge of coding.
      - I focused on it afterwards.
    - Conclusion
      - Code Bronze Medal
1. [Market Prediction XGBoost with GPU Modified](https://github.com/satorushibata0627/Kaggle/blob/main/Kaggle_Python3_Bronze_Medal_Market_Prediction_XGBoost_with_GPU_Modified.ipynb)
    - Lead sentences
      - Performance comparison with LightGBM by XGBoost Optimization.
      - LightGBM takes the cake!
    - Issue
      - I seen good results with XGBoost sometimes.
    - Significance
      - Simulate on Models other than LightGBM and search for Optimized Model.
    - Purpose
      - Score improvement by XGBoost
    - Methodology
      - GPU Implementation into XGBoost Optimization
    - Results
      - Score: 3308.824 (Outside of Medal Zone)
      - Code: 8 Points
    - Considerations
      - XGBoost is easy to implement due to its many precedents.
      - LightGBM is superior in performance comparison, which led me to focus on LightGBM.
    - Conclusion
      - Code Bronze Medal
1. [Cassava Leaf Disease Best Keras CNN Tuning](https://github.com/satorushibata0627/Kaggle/blob/main/Kaggle_Python3_Bronze_Medal_Cassava_Leaf_Disease_Best_Keras_CNN_Tuning.ipynb)
     - Lead sentences
        - I also participated in a competition on image analysis, challenging myself with raw data of various properties!
        - I was left with some issues on the theoretical side, which gave me an opportunity to work from theoretical books.
     - Issue
        - I would like to try my hand at image analysis and find out what I am good at.
     - Significance
        - I want to gain experience in Keras implementation.
        - Deepen my understanding CNN
     - Purpose
        - I learn to understand and implement acoustic analysis and image analysis.
     - Methodology
        - I complemented the advanced submission code.
     - Results
        - Score: Accuracy = 0.885
        - Code: 18 Points
     - Considerations
        - Theoretical aspects of acoustic analysis and image analysis remained a challenge.
        - An opportunity to raise awareness to need to start with a survey of theoretical papers.
    - Conclusion
        - Code Bronze Medal
1. [RFCX Residual Network with TPU Customized](https://github.com/satorushibata0627/Kaggle/blob/main/Kaggle_Python3_Bronze_Medal_RFCX_Residual_Network_with_TPU_Customized.ipynb)
    - Lead sentences
      - I also participated in a competition for acoustic analysis, and tried my hand at raw data of various properties!
      - I was left with some issues on the theoretical side, which gave me an opportunity to work from theoretical books.
    - Issue
      - I would like to try my hand at acoustic analysis and find out what I am good at.
    - Significance
      - I want to gain experience in Keras implementation.
      - Deepen my understanding CNN
    - Purpose
      - I learn to understand and implement acoustic analysis and image analysis.
    - Methodology
      - I complemented the advanced submission code.
    - Results
      - Score: 0.772
      - Code: 12 Points
    - Considerations
      - Theoretical aspects of acoustic analysis and image analysis remained a challenge.
      - An opportunity to raise awareness to need to start with a survey of theoretical papers.
    - Conclusion
      - Code Bronze Medal
1. [Research with Customized Sharp Weighted](https://github.com/satorushibata0627/Kaggle/blob/main/Kaggle_Python3_Bronze_Medal_Research_with_Customized_Sharp_Weighted.ipynb)
    - Lead sentences
      - Work on Custom Metrics Clarification and systematization of Hyperparameters Optimization in LightGBM.
      - An each milestone optimization object generation is still important!
    - Issue
      - Private Custom Metrics were used as an Evaluation Function.
    - Significance
      - Improve prediction accuracy by elucidating Private Custom Metrics.
      - Reproducibility will be determined based on the Evaluation Function.
    - Purpose
      - Custom Metrics Clarification
    - Methodology
      - LightGMB High Parameter Optimization
      - Systematization with Custom Metrics Decoding Examples
    - Results
      - Generate each Parameter Optimization Object
      - Code: 6 Points
    - Consideration
      - Importance of each milestone optimization object generation was reaffirmed.
    - Conclusion
      - Code Bronze Medal
1. [Optimize CatBoost HyperParameter with Optuna and GPU](https://github.com/satorushibata0627/Kaggle/blob/main/Kaggle_Python3_Bronze_Medal_Optimize_CatBoost_HyperParameter_with_Optuna_and_GPU.ipynb)
    - Lead sentences
      - Performance comparison was performed on optimized Ensemble Learning.
      - LightGBM won the prediction accuracy.
    - Issue
      - I was new to CatBoost and wanted to compare performance with LightGBM.
    - Significance
      - Performance comparison of Ensemble Learning: LightGBM, XGBoost, CatBoost
    - Purpose
      - Algorithm selection for Prediction Models
    - Methodology
      - Hyper-parameter optimization
      - CatBoost implementation
    - Results
      - Score: AUC = 0.500
      - Code: 17 Points
    - Consideration
      - At the baseline model stage, I gave the edge to LightGBM.
    - Conclusion
      - Code Bronze Medal
1. [LightGBM on Lyft Tabular Data added Inference and Tuning](https://github.com/satorushibata0627/Kaggle/blob/main/Kaggle_Python3_Bronze_Medal_LGBM_on_Lyft_Tabular_Data_Inference_Tuning.ipynb)
    - Lead sentences
      - Regression Prediction of LightGBM with Grid Search and Multiple Evaluation Functions
      - A harvest that uncovered all sorts of challenges!
    - Issue
      - Regression Problem for Table Data Related to Automated Driving
    - Significance
      - I want to work on Regression Prediction with LightGBM.
      - Gain further development experiences.
      - Implement multiple evaluation functions to improve accuracy.
    - Purpose
      - Improving accuracy of Regression Prediction
    - Methodology
      - Set evaluation functions of LightGBM in MSE and RMSE
      - Parameter search by grid search
    - Results
      - Score: 356.084
      - Code: 10 Points
    - Considerations
      - Grid search shown that hyperparameter optimization is inefficient.
      - I reaffirmed the need to use feature engineering and inference.
    - Conclusion
      - Code Bronze Medal
1. [COVID-19 with H2OAutoML Baseline Model](https://github.com/satorushibata0627/Kaggle/blob/main/Kaggle_Python3_Bronze_Medal_COVID-19_with_H2OAutoML_Baseline.ipynb)
    - Lead sentences
      - Experimented with AutoML performance, but found the original to be more powerful!
      - This led to the original development of the LightGBM optimization.
    - Issue
      - COVID-19 infection explosion and new global challenges.
    - Significance
      - Improvement of coding techniques for anonymized Table data
      - Accumulate experiences using AutoML
    - Purpose
      - Optimization Regression Prediction with AutoML
    - Methodology
      - Set RMSLE as evaluation function for Regression Prediction with H2O.
      - Extract the optimized Regression Prediction Models: Deep Learning, XGBoost, GLM, GBM
    - Results
      - Score: RMSLE = 0.086
      - Code: 6 Points
    - Considerations
      - Original development was more powerful than H2OAutoML.
      - Opportunity to work on Optimized Regression Prediction with LightGBM.
    - Conclusion
      - Code Bronze Medal
1. [Optimized Predictive Model with H2OAutoML](https://github.com/satorushibata0627/Kaggle/blob/main/Kaggle_Python3_Bronze_Medal_Optimized_Predictive_Model_with_H2OAutoML.ipynb)
    - Lead sentences
      - Even in Binary Classification, AutoML was found to be inferior to proprietary!
      - It is thought that the difference was due to Preprocess and Feature Engineering.
    - Issue
      - Regression Prediction by H2OAutoML was inferior to original development.
    - Significance
      - It was unclear whether results would be similar to Regression Prediction.
    - Purpose
      - Experiment on H2OAutoML in Binary Classification.
    - Methodology
      - Set RMSLE as the evaluation function for Binary Classification with H2O.
      - Extract the Optimized Binary Classification Models: Deep Learning, XGBoost, GLM, GBM
    - Results
      - Score: AUC = 0.850
      - Code: 5 Points
    - Considerations
      - The performance was higher than that of Regression Prediction Case.
      - Process and Feature Engineering itself is not automated.
      - It has to be developed independently.
    - Conclusion
      - Code Bronze Medal

### 実証研究
#### [エネルギー・気候変動・日本経済2050年問題向け動学的パネル因果推論モデリング](https://github.com/Satoru-Shibata-JP/Kaggler/raw/main/%E5%85%AC%E7%9B%8A%E8%B2%A1%E5%9B%A3%E6%B3%95%E4%BA%BA%E3%81%BF%E3%81%9A%E3%81%BB%E5%AD%A6%E8%A1%93%E6%8C%AF%E8%88%88%E8%B2%A1%E5%9B%A3%E7%AC%AC61%E5%9B%9E%E6%87%B8%E8%B3%9E%E8%AB%96%E6%96%87.pdf)

##### 要旨
　本稿は、エネルギー・気候変動・日本経済2050年問題向け動的マクロ計量モデルモデリングにより課題解決手段としてESG (Environmental, Social, and, Corporate Governance)投資利用を提案するものである。
<br>
  政府統計等からローデータを収集した後、偏グレンジャー因果性に統計的有意性を確認する一方、共和分検定の結果、パネルVARモデルによる動的直接相関係数を適当と判断し、その標準偏回帰係数に平均・分散を求めて幾何ブラウン運動を以て2050年までのシミュレーション行とともに、LSTMを基に精度評価を行った。
<br>
　その結果、ESG投資利用に次項を例示した。
<br> 
1. 再生可能エネルギー普及策推進
1. CCS火力技術開発・導入
1. シェール資源輸入・産業利用
1. 気候変動対策国際合意形成推進産学連携

###### キーワード
- 共和分検定
- 単位根検定
- 偏グレンジャー因果性検定
- 非直交化インパルス応答関数
- パネルVAR (Vector Auto Regressive)モデル
- 幾何ブラウン運動
- 確率偏微分方程式
- LSTM (Long Short Term Memory)
- 多重共線性

#### [パネルVAR-SPDEモデルとLSTM精度評価アルゴリズム開発資料](https://github.com/Satoru-Shibata-JP/Kaggler/blob/main/Dev_Doc_PVARSPDE_LSTM.ipynb)
##### 要旨
　エネルギー・気候変動・日本経済の各指標における因果関係を解明するためのアルゴリズム設計・実装を目的とする。
<br>
　気候変動対策改正有無に基づくパネルデータを形成し、VARモデル・偏グレンジャー因果性検定等により因果推論を実証した。
<br>
　加えて、この構造方程式を解析して2050年までの可視化シミュレーションを実施した。
<br>
　尚、論文作成において、計算ロジックを再現し、描画したグラフを本文に貼り付けるために実装した。

##### 実装設計
###### 目標
- 動的パネル因果推論
- 因果構造可視化
- シミュレーション精度評価

###### 前提
- メモリ容量最大化
- 必要なライブラリの読込
- 関数定義
- ローデータ目視確認

###### 実行手順
1. 誤差項調整
1. 多重共線性の実証分析
1. 無相関検定
1. 単位根検定(ADF検定)
1. 共和分検定
1. 偏グレンジャー因果性検定と非直交化インパルス応答関数
1. パネルVARモデルによる動的直接相関係数の導出
1. 幾何ブラウン運動
1. LSTM
1. グラフ描画
  
###### 作動環境
  - R
  - Jupyter Notebook

#### [上智大学修士論文（環境経済学)](https://github.com/Satoru-Shibata-JP/Kaggler/raw/main/%E4%B8%8A%E6%99%BA%E5%A4%A7%E5%AD%A6%E7%92%B0%E5%A2%83%E5%AD%A6%E4%BF%AE%E5%A3%AB%E5%AD%A6%E4%BD%8D%E8%AB%96%E6%96%87.pdf)
##### 要旨
　日本エネルギー改革策の持続可能性に関し、ドイツ・イギリス・フランスを先行事例とし、エネルギー政策に関連する政府統計を基に、各国に対数線形重回帰モデルを形成した。
 <br>
　この結果から、日本のエネルギー改革策の持続可能性は必ずしも高くないと知見を得るとともに、再生可能エネルギーへの公共投資を第二次安倍内閣の新・経済政策第三の矢とすることを提言した。

##### 分析手法
- 対数線形重回帰分析
- 主成分分析
- 分散分析
- 偏微分
- 重積分
- グラフ描画
- 文献調査

##### 実装
- Microsoft Excel
- Microsoft Word

#### [実証分析参考資料](https://github.com/Satoru-Shibata-JP/Publication/raw/main/%E5%AE%9F%E8%A8%BC%E5%88%86%E6%9E%90%E5%8F%82%E8%80%83%E8%B3%87%E6%96%99.pdf)
- 実証分析を用いるうえで必要な知識を要約した資料である。
- Microsoft PowerPointにて作成した。
