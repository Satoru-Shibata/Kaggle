# Executive Summary
### About the Author
- Name: Satoru Shibata
- Job: Data Scientist at AVANT CORPORATION
  - Title: [Kaggle Triple Expert Top 0.2%, 0.3%, 1% (Python/R)](https://github.com/satorushibata0627/Kaggle/raw/main/Kaggle_Triple_Expert_Evidence.pdf)
  - Degree: MEcon at Sophia University
- Current Status: Retirement to focus on my job

### Score Table
| Departments      | Top Levels| Highest Rank | Awarded Medals     | 
| :---:            | :-------: | :----------: | :----------------: |
| **Code**         | 0.2%      | 317/161,898  | 3 Silver 13 Bronze |
| **Discussion**   | 0.3%      | 588/188,433  | 100 Bronze         |
| **Datasets**     | 1%        | 354/34,643   | 3 Bronze           |
| **Competitions** | 20-30%

### 16 Awarded Analytical Reports Abstract
- Python
- Jupyter Notebook
#### 3 Silver Medals
1. [Optimized LightGBM with Optuna adding SAKT Model](https://github.com/satorushibata0627/Kaggle/blob/main/Kaggle_Python3_Silver_Medal_Optimized_LightGBM_with_Optuna_adding_SAKT_Model.ipynb)
    - Lead sentences
      - Submitted code using two Ensemble Learning Methods.
      - Used 100 million rows of training data for prediction on a 16GB Kernel removing unnecessary objects.
    - Issue
      - Algorithms for TOEIC Learning Applications
    - Significance
      - Predict percentage of correct answers based on user's behavioral history.
      - User's percentage of correct answers will increase with the number of problems solved.
    - Purpose
      - Optimize Binary Classification for AUC.
    - Methodology:
      - Ensemble Learning of LightGBM and SAKT
      - Hyperparameter Optimization with Optuna
    - Results
      - Score: AUC = 0.781
      - Code: 31 Points
    - Considerations
      - Obsessed with Models, Feature Engineering Remains a Challenge.
      - Systematizing Multiple Models will also be a challenge in the future.
    - Conclusion
      - Code Silver Medal
2. [LightGBM Classifier and Logistic Regression Report](https://github.com/satorushibata0627/Kaggle/blob/main/Kaggle_Python3_Silver_Medal_LightGBM_Classifier_and_Logistic_Regression_Report.ipynb)
    - Lead sentences
      - Optimized Classification of Anonymized Raw Data from Stock Market on 16GB Kernel.
      - Contributed code that systematizes Ensemble Learning and Logistic Regression.
    - Issue
      - Utility Function Optimization of Supply and Demand Forecasting in Securities Markets
    - Significance
      - Calculate based on indicators of absence or degree of stock returns.
      - Optimize the behavior of whether to trade or not.
    - Purpose
      - AI Dev for Profit Maximization
    - Methodology
      - Optimal classification of LightGBM
      - Logit Transformation of Purpose Variables Based on Probability Distributions
    - Results
      - Score: 3741.118 (Outside of Medal Zone)
      - Code: 33 Points
    - Considerations
      - Utility function was not fully deciphered
      - Which left some issues for the paper survey.
      - I think that the reporting was appreciated by other Kaggler.
    - Conclusion
      - Code Silver Medal
3. [Optimize LightGBM HyperParameter with Optuna and GPU](https://github.com/satorushibata0627/Kaggle/blob/main/Kaggle_Python3_Silver_Medal_Optimize_LightGBM_HyperParameter_with_Optuna_and_GPU.ipynb)
    - Lead sentences
      - Unprecedented LightGBM Hyperparameter Optimization on GPU!
      - Procedure was annexed and highly evaluated.
    - Issues
      - Preliminaries of “LightGBM Classifier and Logistic Regression Report“ 
    - Significance
      - High Parameter Optimization
      - There were few precedents for LightGBM.
    - Purpose
      - Code submission for optimizing LightGBM Hyperparameter on GPU
    - Methodology
      - A survey of prior case studies using Optuna for LightGBM
      - Procedure Submission
    - Results
      - Run: 953.9s on GPU
      - Code: 31 Points
    - Consideration
      - It can be used for Hyperparameter Optimization in the future.
    - Conclusion
      - Code Silver Medal

#### 13 Bronze Medals
1. [Optimized Logit LightGBM Classifier and CNN Models](https://github.com/satorushibata0627/Kaggle/blob/main/Kaggle_Python3_Bronze_Medal_Optimized_Logit_LightGBM_Classifier_and_CNN_Model.ipynb)
    - Lead sentences
      - Submitted a simulation of Multiple Model Systematization.
      - Based on this failure, I was able to concentrate on LightGBM Optimization and Inference.
    - Issue
      - Exploring Optimization Models
    - Significance
      - Simulation iterations of the Optimization Model
    - Purpose
      - Optimize Utility Function by systematizing Multiple Models.
    - Methodology
      - Applying the Logit Transform to LightGBM
      - Explore combining with CNN
    - Results
      - Score: 3344.738 (Outside of Medal Zone)
      - Code: 15 Points
    - Considerations
      - This code does LightGBM and CNN at the same time, which was prone to overflow.
      - From now on, I will focus on one Model Optimization.
    - Conclusion
      - Code Bronze Medal
3. ["Optimized LightGBM with Optuna"](https://github.com/satorushibata0627/Kaggle/blob/main/Kaggle_Python3_Bronze_Medal_Optimized_LightGBM_with_Optuna.ipynb)
4. ["LightGBM on GPU with Feature Engineering Optuna and Visualization"](https://github.com/satorushibata0627/Kaggle/blob/main/Kaggle_Python3_Bronze_Medal_LightGBM_on_GPU_with_Feature_Engineering_Optuna_and_Visualization.ipynb)
5. ["LightGBM with the Inference and Empirical Analysis"](https://github.com/satorushibata0627/Kaggle/blob/main/Kaggle_Python3_Bronze_Medal_LightGBM_with_the_Inference_and_Empirical_Analysis.ipynb)
6. ["Submission and the Inference of LightGBM"](https://github.com/satorushibata0627/Kaggle/blob/main/Kaggle_Python3_Bronze_Medal_Submission_and_the_Inference_of_LightGBM.ipynb)
7. ["Market Prediction XGBoost with GPU Modified"](https://github.com/satorushibata0627/Kaggle/blob/main/Kaggle_Python3_Bronze_Medal_Market_Prediction_XGBoost_with_GPU_Modified.ipynb)
8. ["Cassava Leaf Disease Best Keras CNN Tuning"](https://github.com/satorushibata0627/Kaggle/blob/main/Kaggle_Python3_Bronze_Medal_Cassava_Leaf_Disease_Best_Keras_CNN_Tuning.ipynb)
9. ["RFCX Residual Network with TPU Customized"](https://github.com/satorushibata0627/Kaggle/blob/main/Kaggle_Python3_Bronze_Medal_RFCX_Residual_Network_with_TPU_Customized.ipynb)
10. ["Research with Customized Sharp Weighted"](https://github.com/satorushibata0627/Kaggle/blob/main/Kaggle_Python3_Bronze_Medal_Research_with_Customized_Sharp_Weighted.ipynb)
11. ["Optimize CatBoost HyperParameter with Optuna and GPU"](https://github.com/satorushibata0627/Kaggle/blob/main/Kaggle_Python3_Bronze_Medal_Optimize_CatBoost_HyperParameter_with_Optuna_and_GPU.ipynb)
12. ["LGBM on Lyft Tabular Data [Inference] + Tuning"](https://github.com/satorushibata0627/Kaggle/blob/main/Kaggle_Python3_Bronze_Medal_LGBM_on_Lyft_Tabular_Data_Inference_Tuning.ipynb)
13. ["COVID-19 with H2OAutoML Baseline"](https://github.com/satorushibata0627/Kaggle/blob/main/Kaggle_Python3_Bronze_Medal_COVID-19_with_H2OAutoML_Baseline.ipynb)
14. ["Optimized Predictive Model with H2OAutoML"](https://github.com/satorushibata0627/Kaggle/blob/main/Kaggle_Python3_Bronze_Medal_Optimized_Predictive_Model_with_H2OAutoML.ipynb)
